---
title: "MNIST example"
author: "He Junhui"
date: '`r Sys.Date()`'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(devtools)
library(kernlab)
library(FLGP)
library(microbenchmark)
library(bigmemory)
load_all()
```

```{r}
#######################################################
# The second example
# MNIST examples
# 70000 * 28 * 28

set.seed(1234)
```

```{r}
# read mnist samples
inputpath = "G:/Phd/Data/MNIST/mnist/"
# xbm.mnist = read.big.matrix(paste0(inputpath, "images.csv"), has.row.names =  TRUE, type = "double", skip = 1)
```


```{r}
# DO PCA
# p = 100
# x.pca.mnist = prcomp(x=xbm.mnist[,], rank. = p)
# write.csv(x=x.pca.mnist$x, file = paste0(inputpath, "images_pca.csv"))
```

```{r}
xbmpca.mnist = read.big.matrix(paste0(inputpath, "images_pca.csv"), has.row.names =  TRUE, type = "double", skip = 1)
y.mnist = read.big.matrix(paste0(inputpath, "labels.csv"), has.row.names = TRUE,
                          type = "double", skip = 1)
```


```{r}
# classification
# divide training samples and testing samples

# n = dim(xbmpca.mnist)[1] # 70000
n = 7000
d = dim(xbmpca.mnist)[2] # 100
m = 100

train.index.mnist = sample.int(n, m); test.index.mnist = c(1:n)[-train.index.mnist]
train.data.mnist = xbmpca.mnist[train.index.mnist,]
test.data.mnist = xbmpca.mnist[test.index.mnist, ]
```

```{r}
y_train.mnist = y.mnist[train.index.mnist]
y_new.mnist = y.mnist[test.index.mnist]
```

+ Classification

```{r}
# squared exponential kernel
t1 = Sys.time()
rbf.model = train_multi_classification(train.data.mnist, y_train.mnist)
res_rbf = predict_multi_classification(test.data.mnist, rbf.model)
t2 = Sys.time()
t2 - t1
pred_rbf = res_rbf$pred
err.rbf = mean(y_new.mnist!=pred_rbf)
err.rbf
# 0.3424638
```

```{r}
mean = res_rbf$posterior$mean
cov = res_rbf$posterior$cov
nll_rbf = negative_log_likelihood(mean, cov, y_new.mnist, "multinomial")
nll_rbf
# 2.88924
```


```{r}
# ksvm
ksvm.mnist = ksvm(x=train.data.mnist, y=as.factor(y_train.mnist), 
                    type="C-svc", scaled = FALSE)
pred_label.ksvm.mnist = predict(ksvm.mnist, test.data.mnist)
err.ksvm.mnist = sum((y_new.mnist!=pred_label.ksvm.mnist)^2)/(n-m)
err.ksvm.mnist
# 0.2957971
```

+ FLGP

```{r}
# hyper parameters
s = 1000; r = 3; K = 50
models = list(subsample="kmeans", kernel="lae", gl="cluster-normalized", root=TRUE)
```

+ LKFLGP
```{r}
# LKFLGP
models$subsample = "kmeans"; models$gl = "cluster-normalized"
```


```{r}
t1 = Sys.time()
res_lkflag.mnist = fit_lae_logit_mult_gp_rcpp(train.data.mnist, y_train.mnist, test.data.mnist, s, r, K, models = models)
t2 = Sys.time()
t2 - t1
# Time difference of 20.44336 secs
```

```{r}
y_lkflag.mnist = res_lkflag.mnist$Y_pred
err_lkflag.mnist = sum((y_new.mnist!=y_lkflag.mnist$test)^2)/(n-m)
err_lkflag.mnist
# 0.1663768
```

```{r}
mean = res_lkflag.mnist$posterior$mean
cov = res_lkflag.mnist$posterior$cov
nll_lk = negative_log_likelihood(mean, cov, y_new.mnist, "multinomial")
nll_lk
# 1.930565
```


+ LRFLGP
```{r}
# LKFLGP
models$subsample = "random"; models$gl = "normalized"
```


```{r}
t1 = Sys.time()
res_lrflag.mnist = fit_lae_logit_mult_gp_rcpp(train.data.mnist, y_train.mnist, test.data.mnist, s, r, K, models = models)
t2 = Sys.time()
t2 - t1
# Time difference of 5.835267 secs
```

```{r}
y_lrflag.mnist = res_lrflag.mnist$Y_pred
err_lrflag.mnist = sum((y_new.mnist!=y_lrflag.mnist$test)^2)/(n-m)
err_lrflag.mnist
# 0.2334478
```

```{r}
mean = res_lrflag.mnist$posterior$mean
cov = res_lrflag.mnist$posterior$cov
nll_lr = negative_log_likelihood(mean, cov, y_new.mnist, "multinomial")
nll_lr
# 2.105812
```

+ SKFLGP
```{r}
# SKFLGP
models$subsample = "kmeans"; models$gl = "cluster-normalized"
```

```{r}
t1 = Sys.time()
res_skflag.mnist = fit_se_logit_mult_gp_rcpp(train.data.mnist, y_train.mnist, test.data.mnist, s, r, K, models = models)
t2 = Sys.time()
t2 - t1
# Time difference of 30.50837 secs
```

```{r}
y_skflag.mnist = res_skflag.mnist$Y_pred
err_skflag.mnist = sum((y_new.mnist!=y_skflag.mnist$test)^2)/(n-m)
err_skflag.mnist
# 0.1569565
```

```{r}
mean = res_skflag.mnist$posterior$mean
cov = res_skflag.mnist$posterior$cov
nll_sk = negative_log_likelihood(mean, cov, y_new.mnist, "multinomial")
nll_sk
# 1.917983
```

+ SRFLGP
```{r}
# SRFLGP
models$subsample = "random"; models$gl = "normalized"
```

```{r}
t1 = Sys.time()
res_srflag.mnist = fit_se_logit_mult_gp_rcpp(train.data.mnist, y_train.mnist, test.data.mnist, s, r, K, models = models)
t2 = Sys.time()
t2 - t1
# Time difference of 14.88994 secs
```

```{r}
y_srflag.mnist = res_srflag.mnist$Y_pred
err_srflag.mnist = sum((y_new.mnist!=y_srflag.mnist$test)^2)/(n-m)
err_srflag.mnist
# 0.1885265
```

```{r}
mean = res_srflag.mnist$posterior$mean
cov = res_srflag.mnist$posterior$cov
nll_sr = negative_log_likelihood(mean, cov, y_new.mnist, "multinomial")
nll_sr
# 2.013103
```

+ Nystrom extension
```{r}
# Nystrom extension
t1 = Sys.time()
bwd = exp(seq(log(0.1),log(10),length.out=10))
res_nystrom.mnist = fit_nystrom_logit_mult_gp_rcpp(train.data.mnist, y_train.mnist, test.data.mnist, s, K, a2s = bwd)
t2 = Sys.time()
t2 - t1
# Time difference of 2.402349 mins
```

```{r}
y_nystrom.mnist = res_nystrom.mnist$Y_pred
err_nystrom.mnist = sum((y_new.mnist!=y_nystrom.mnist$test)^2)/(n-m)
err_nystrom.mnist
# 0.3088406
```

```{r}
mean = res_nystrom.mnist$posterior$mean
cov = res_nystrom.mnist$posterior$cov
nll_nystrom = negative_log_likelihood(mean, cov, y_new.mnist, "multinomial")
nll_nystrom
# 2.43722
```

+ GLGP
```{r}
# GLGP
t1 = Sys.time()
bwd = exp(seq(log(0.1),log(10),length.out=10))
res_gl.mnist = fit_gl_logit_mult_gp_rcpp(train.data.mnist, y_train.mnist, test.data.mnist, K,
                                         sparse = FALSE, a2s = bwd)
t2 = Sys.time()
t2 - t1
# Time difference of 7.454023 mins
```

```{r}
y_gl.mnist = res_gl.mnist$Y_pred
err_gl.mnist = sum((y_new.mnist!=y_gl.mnist$test)^2)/(n-m)
err_gl.mnist
# 0.3382609
```

```{r}
mean = res_gl.mnist$posterior$mean
cov = res_gl.mnist$posterior$cov
nll_gl = negative_log_likelihood(mean, cov, y_new.mnist, "multinomial")
nll_gl
# 2.482875
```

+ SparseGLGP
```{r}
# SparseGLGP
t1 = Sys.time()
bwd = exp(seq(log(0.3),log(10),length.out=10))
res_glsp.mnist = fit_gl_logit_mult_gp_rcpp(train.data.mnist, y_train.mnist, test.data.mnist, K, threshold = 0.01, sparse = TRUE, a2s = bwd)
t2 = Sys.time()
t2 - t1
# Time difference of 7.454023 mins
```

```{r}
y_glsp.mnist = res_glsp.mnist$Y_pred
err_glsp.mnist = sum((y_new.mnist!=y_glsp.mnist$test)^2)/(n-m)
err_glsp.mnist
# 0.1605866
```

```{r}
mean = res_glsp.mnist$posterior$mean
cov = res_glsp.mnist$posterior$cov
nll_glsp = negative_log_likelihood(mean, cov, y_new.mnist, "multinomial")
nll_glsp
```
